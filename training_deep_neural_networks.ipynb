{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_deep_neural_networks.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 10 showed us artificial neural networks and we trained a deep one for the first time.\n",
    "\n",
    "# Some probelms you may find in dnn is: \n",
    "# Vanishing gradients or exploding gradients (both making lower layers hard to train)\n",
    "# Not enough training data means too costly to label\n",
    "# Extremely slow training (some times)\n",
    "# so many parameters means we could overfit\n",
    "\n",
    "# In this chapter we will study each of these problems and how we could get around them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanishing and Exploding Gradients\n",
    "\n",
    "# From chaper 10 we know back propagation works by going from the output layer to the input \n",
    "# layer. Propagating the errors as well! Overall DNN's suffer from unstable gradients and thus\n",
    "# different layers many learn at different speeds.\n",
    "\n",
    "# A solution preposed by Glorot and Bengio. The connetion weights must be initialised randomly\n",
    "# as described below, where: fan_avg = (fan_in +fan_out)/2\n",
    "\n",
    "# Glorot initialization (when using logistic activiation function)\n",
    "# normal distribution wiht mean 0 and varience sigma**2 = 1/fan_avg\n",
    "# or a uniform dist between -r and r where r = sqrt(3/fan_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initialization strategy for the ReLO activation fuction is cometimes called He initialization\n",
    "# The SELU activation function will be explained later in the chapter (init = LeCun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fe152004950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default keras uses Glorot init wiht uniform ditributions. You can change this to He with:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fe158dc8310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want He init wiht a uniform distribution but based pn fan_avg rather than fan_in,\n",
    "# you can use the VarianceScaling init as follows:\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg',\n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-10c2bdbbecb0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-10c2bdbbecb0>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    keras.layers.Dense(10, kernel-initializer=\"he_normal\"),\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# A better version of relu is the leaky relu which has a \"leak\" in negative values (look online)\n",
    "# example of use:\n",
    "model = keras.models.Sequential([\n",
    "    ...\n",
    "    keras.layers.Dense(10, kernel-initializer=\"he_normal\"),\n",
    "    keras.layeres.LeakyReLU(alpha=0.2),\n",
    "    ...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for PReLU replace LeakyReLU with PReLU. This is where alpha is no longer a hyper param\n",
    "# instead it is a paramerter that changes with the model. It is great for large image datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalisation\n",
    "\n",
    "# Gradient problems can still come through during training. Ioffe and Szegedy proposed BN to\n",
    "# address this problem.\n",
    "\n",
    "# It consists of adsing an operation in the model just before or after the activation function\n",
    "# of each hidden layer.The function will simply zero-centre and normalize each input. In other\n",
    "# words if you add a BN layer as the first layer of your nn then you dont need to standardise\n",
    "# your data sets (eg standard scaler)\n",
    "\n",
    "# See equation 11-3 \n",
    "\n",
    "# During training the BN standardises the inputs, rescales and offsets them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main negatives wiht BN is firstly that it adds a layer of complexity and worse than this is\n",
    "# that it adds run time. But this can be avoided. Now let us impleement BN using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
